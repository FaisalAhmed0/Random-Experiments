{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as opt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the policy model\n",
    "class MLP_policy(nn.Module):\n",
    "  def __init__(self, n_input, n_hiddens, n_actions, action_type=\"discrete\"):\n",
    "    super().__init__()\n",
    "    self.action_type = action_type\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(n_input, n_hiddens[0]))\n",
    "    layers.append(nn.ReLU())\n",
    "    for i in range(len(n_hiddens)-1):\n",
    "      layers.append( nn.Linear(n_hiddens[i], n_hiddens[i+1]) )\n",
    "      layers.append( nn.ReLU() )\n",
    "    \n",
    "    self.model = nn.Sequential(*layers)\n",
    "    if action_type == \"discrete\":\n",
    "      self.dis_actions = nn.Linear(n_hiddens[-1], n_actions)\n",
    "    else:\n",
    "      self.mean = nn.Linear(n_hiddens[-1], n_actions)\n",
    "      self.log_var = nn.Linear(n_hiddens[-1], n_actions)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.action_type == \"discrete\":\n",
    "      return self.dis_actions(self.model(x))\n",
    "    else:\n",
    "      x = self.model(x)\n",
    "      mean = self.mean(x)\n",
    "      log_var = self.log_var(x)\n",
    "      return mean, log_var\n",
    "  \n",
    "  def sample_action(self, x):\n",
    "    if self.action_type == \"discrete\":\n",
    "      x = self.forward(x)\n",
    "      self.actions_probs = F.softmax(x, dim=-1)\n",
    "      action = torch.multinomial(self.actions_probs, 1)\n",
    "      log_prob = F.log_softmax(x)[action] \n",
    "      return action, log_prob\n",
    "    else:\n",
    "      mean, log_var = self.forward(x)\n",
    "      covar = torch.exp(log_var)\n",
    "      self.actions_dist = MultivariateNormal(mean, torch.diag(covar))\n",
    "      action = self.actions_dist.sample()\n",
    "      log = self.actions_dist.log_prob(action)\n",
    "      return action, log\n",
    "\n",
    "  def entropy(self):\n",
    "    if self.action_type == \"discrete\":\n",
    "      return Categorical(self.actions_probs).entropy()\n",
    "    else:\n",
    "      return self.actions_dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_env_name = \"CartPole-v0\"\n",
    "source_env = gym.make(source_env_name)\n",
    "\n",
    "target_env_name = \"InvertedPendulum-v2\"\n",
    "target_env = gym.make(target_env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "action_type = \"discrete\"\n",
    "if action_type == \"discrete\":\n",
    "        model = MLP_policy(source_env.observation_space.shape[0], [64, 32], source_env.action_space.n, \"discrete\")\n",
    "else:\n",
    "    model = MLP_policy(source_env.observation_space.shape[0], [64, 32], source_env.action_space.shape[0], \"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trainied model on the source env\n",
    "model.load_state_dict(torch.load(\"best_model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_policy(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (dis_actions): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_policy(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (dis_actions): None\n",
       "  (mean): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (log_var): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the last layer of the model\n",
    "model.action_type = \"Continuous\"\n",
    "model.dis_actions = None\n",
    "model.mean = nn.Linear(32, target_env.action_space.shape[0])\n",
    "model.log_var = nn.Linear(32, target_env.action_space.shape[0])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 51202), started 0:06:23 ago. (Use '!kill 51202' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for the reward to go\n",
    "def reward_to_go(rewards, gamma=0.99):\n",
    "  cum_rewards = []\n",
    "  sum_rewards = 0\n",
    "  for r in reversed(rewards):\n",
    "      sum_rewards = gamma * sum_rewards + r\n",
    "      cum_rewards.append( sum_rewards )\n",
    "  cum_rewards.reverse()\n",
    "  return torch.tensor(cum_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_norm(net):\n",
    "  '''\n",
    "  gradient_norm(net)\n",
    "  This function calulate the gradient norm of a neural network model.\n",
    "  net: network model\n",
    "  '''\n",
    "  total_norm = 0\n",
    "  for param in net.parameters():\n",
    "    param_norm = param.grad.detach().data.norm(2)\n",
    "    total_norm += param_norm.item() ** 2\n",
    "  return total_norm**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(sw, iter, **kwargs):\n",
    "  '''\n",
    "  Helper function to add logs to tensorboard\n",
    "  '''\n",
    "  # print(\"Here\")\n",
    "  for k in kwargs:\n",
    "    sw.add_scalar(k, kwargs[k], iter+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configirations\n",
    "# policy learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# iterations\n",
    "iterations = 87\n",
    "\n",
    "# gamma\n",
    "gamma = 0.99\n",
    "\n",
    "# baseline type\n",
    "# baseline type 1: avergae reward, 2: value function\n",
    "baseline = None\n",
    "\n",
    "# use entropy regulrization\n",
    "entropy = False\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f999a67d110>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:1, reward: 6.890625, batch loss: 44.40943908691406\n",
      "Iteration:2, reward: 7.75, batch loss: 57.23524475097656\n",
      "Iteration:3, reward: 8.703125, batch loss: 76.3541030883789\n",
      "Iteration:4, reward: 7.59375, batch loss: 56.46784210205078\n",
      "Iteration:5, reward: 8.015625, batch loss: 58.748802185058594\n",
      "Iteration:6, reward: 8.65625, batch loss: 60.22014617919922\n",
      "Iteration:7, reward: 7.671875, batch loss: 53.97249984741211\n",
      "Iteration:8, reward: 9.21875, batch loss: 80.33733367919922\n",
      "Iteration:9, reward: 9.3125, batch loss: 82.07076263427734\n",
      "Iteration:10, reward: 9.46875, batch loss: 85.37443542480469\n",
      "Iteration:11, reward: 9.359375, batch loss: 83.05612182617188\n",
      "Iteration:12, reward: 9.421875, batch loss: 81.64006042480469\n",
      "Iteration:13, reward: 10.78125, batch loss: 99.58782958984375\n",
      "Iteration:14, reward: 10.328125, batch loss: 95.90492248535156\n",
      "Iteration:15, reward: 10.109375, batch loss: 88.2119369506836\n",
      "Iteration:16, reward: 9.59375, batch loss: 82.52635955810547\n",
      "Iteration:17, reward: 10.375, batch loss: 97.39227294921875\n",
      "Iteration:18, reward: 10.578125, batch loss: 98.89037322998047\n",
      "Iteration:19, reward: 10.296875, batch loss: 85.3226547241211\n",
      "Iteration:20, reward: 12.671875, batch loss: 133.64993286132812\n",
      "Iteration:21, reward: 10.96875, batch loss: 99.88551330566406\n",
      "Iteration:22, reward: 11.78125, batch loss: 115.69866180419922\n",
      "Iteration:23, reward: 12.765625, batch loss: 130.91226196289062\n",
      "Iteration:24, reward: 11.703125, batch loss: 129.69688415527344\n",
      "Iteration:25, reward: 13.265625, batch loss: 145.5247344970703\n",
      "Iteration:26, reward: 13.375, batch loss: 142.55511474609375\n",
      "Iteration:27, reward: 14.609375, batch loss: 162.792236328125\n",
      "Iteration:28, reward: 15.75, batch loss: 204.23838806152344\n",
      "Iteration:29, reward: 14.484375, batch loss: 169.59214782714844\n",
      "Iteration:30, reward: 17.9375, batch loss: 267.8638000488281\n",
      "Iteration:31, reward: 15.34375, batch loss: 177.46302795410156\n",
      "Iteration:32, reward: 16.4375, batch loss: 226.75442504882812\n",
      "Iteration:33, reward: 15.34375, batch loss: 187.12181091308594\n",
      "Iteration:34, reward: 18.9375, batch loss: 282.2369079589844\n",
      "Iteration:35, reward: 16.84375, batch loss: 228.61422729492188\n",
      "Iteration:36, reward: 16.71875, batch loss: 204.9794158935547\n",
      "Iteration:37, reward: 18.109375, batch loss: 239.12844848632812\n",
      "Iteration:38, reward: 21.25, batch loss: 335.5368347167969\n",
      "Iteration:39, reward: 20.21875, batch loss: 286.5080871582031\n",
      "Iteration:40, reward: 25.6875, batch loss: 432.03143310546875\n",
      "Iteration:41, reward: 22.609375, batch loss: 355.5909118652344\n",
      "Iteration:42, reward: 23.03125, batch loss: 347.55267333984375\n",
      "Iteration:43, reward: 25.203125, batch loss: 449.9764404296875\n",
      "Iteration:44, reward: 22.203125, batch loss: 328.08758544921875\n",
      "Iteration:45, reward: 24.203125, batch loss: 368.4106140136719\n",
      "Iteration:46, reward: 26.921875, batch loss: 430.22503662109375\n",
      "Iteration:47, reward: 25.796875, batch loss: 422.5440673828125\n",
      "Iteration:48, reward: 30.296875, batch loss: 547.63916015625\n",
      "Iteration:49, reward: 28.3125, batch loss: 507.071044921875\n",
      "Iteration:50, reward: 31.359375, batch loss: 568.677734375\n",
      "Iteration:51, reward: 30.375, batch loss: 540.5022583007812\n",
      "Iteration:52, reward: 31.59375, batch loss: 571.0699462890625\n",
      "Iteration:53, reward: 31.40625, batch loss: 554.3761596679688\n",
      "Iteration:54, reward: 31.96875, batch loss: 541.0065307617188\n",
      "Iteration:55, reward: 32.6875, batch loss: 592.0230712890625\n",
      "Iteration:56, reward: 34.734375, batch loss: 633.5863647460938\n",
      "Iteration:57, reward: 34.6875, batch loss: 667.7979736328125\n",
      "Iteration:58, reward: 39.5, batch loss: 767.3251953125\n",
      "Iteration:59, reward: 37.734375, batch loss: 762.500244140625\n",
      "Iteration:60, reward: 37.0625, batch loss: 685.43310546875\n",
      "Iteration:61, reward: 37.265625, batch loss: 726.5712280273438\n",
      "Iteration:62, reward: 40.0625, batch loss: 853.552001953125\n",
      "Iteration:63, reward: 38.9375, batch loss: 716.967041015625\n",
      "Iteration:64, reward: 44.421875, batch loss: 928.764404296875\n",
      "Iteration:65, reward: 47.15625, batch loss: 1057.3388671875\n",
      "Iteration:66, reward: 50.9375, batch loss: 1261.3934326171875\n",
      "Iteration:67, reward: 52.3125, batch loss: 1239.380126953125\n",
      "Iteration:68, reward: 53.96875, batch loss: 1307.7694091796875\n",
      "Iteration:69, reward: 51.59375, batch loss: 1196.2900390625\n",
      "Iteration:70, reward: 63.984375, batch loss: 1662.8525390625\n",
      "Iteration:71, reward: 60.53125, batch loss: 1532.349609375\n",
      "Iteration:72, reward: 64.90625, batch loss: 1679.0758056640625\n",
      "Iteration:73, reward: 74.1875, batch loss: 2010.1904296875\n",
      "Iteration:74, reward: 73.953125, batch loss: 1990.275146484375\n",
      "Iteration:75, reward: 71.859375, batch loss: 1860.6732177734375\n",
      "Iteration:76, reward: 83.453125, batch loss: 2430.96142578125\n",
      "Iteration:77, reward: 89.828125, batch loss: 2732.106689453125\n",
      "Iteration:78, reward: 90.1875, batch loss: 2633.811279296875\n",
      "Iteration:79, reward: 97.828125, batch loss: 2914.65673828125\n",
      "Iteration:80, reward: 106.5625, batch loss: 3317.15380859375\n",
      "Iteration:81, reward: 121.484375, batch loss: 4047.019775390625\n",
      "Iteration:82, reward: 122.609375, batch loss: 3982.8916015625\n",
      "Iteration:83, reward: 123.890625, batch loss: 4048.59912109375\n",
      "Iteration:84, reward: 140.453125, batch loss: 4889.138671875\n",
      "Iteration:85, reward: 137.296875, batch loss: 4758.2197265625\n",
      "Iteration:86, reward: 141.140625, batch loss: 5062.08056640625\n",
      "Iteration:87, reward: 159.296875, batch loss: 5854.58642578125\n",
      "Iteration:88, reward: 183.203125, batch loss: 7174.27490234375\n",
      "Iteration:89, reward: 146.078125, batch loss: 5152.75537109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-963961d6dde7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;31m# optimize the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m   \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run without transfer\n",
    "# training loop\n",
    "# create the environment\n",
    "target_env = gym.make(target_env_name)\n",
    "\n",
    "# create the policy model\n",
    "action_type = \"discrete\" if type(gym.make(target_env_name).action_space) == gym.spaces.Discrete else \"continuous\"\n",
    "if action_type == \"discrete\":\n",
    "  model = MLP_policy(target_env.observation_space.shape[0], [64, 32], target_env.action_space.n, action_type)\n",
    "else:\n",
    "  model = MLP_policy(target_env.observation_space.shape[0], [64, 32], target_env.action_space.shape[0], action_type)\n",
    "# model\n",
    "optimizer = opt.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# if baseline == 2:\n",
    "#   # create the value model\n",
    "#   model_v = MLP_value(env.observation_space.shape[0], [64])\n",
    "#   # create the optimizer\n",
    "#   value_optimizer = opt.Adam(model_v.parameters(), lr=lr)\n",
    "\n",
    "# create a summaryWriter\n",
    "sw = SummaryWriter(comment=f\"env_name:{target_env_name}, lr_={lr}, batch_size={batch_size}, baseline: {baseline}, entropy: {entropy}\")\n",
    "temp = 0.1\n",
    "\n",
    "avg_reward_baseline = None\n",
    "\n",
    "for iter in range(iterations):\n",
    "  rewards_avg = 0\n",
    "  entropy_avg = 0\n",
    "\n",
    "  batch_loss = torch.zeros(1)\n",
    "\n",
    "  if baseline == 2:\n",
    "    value_batch_loss = torch.zeros(1)\n",
    "\n",
    "\n",
    "  max_reward = -1000000\n",
    "  min_reward = 1000000\n",
    "  \n",
    "  for traj in range(batch_size):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    state = target_env.reset()\n",
    "    entropies = []\n",
    "    value_baseline = []\n",
    "    done = False\n",
    "    while not done:\n",
    "      state_t = torch.FloatTensor(state)\n",
    "      action, log_prob = model.sample_action(state_t)\n",
    "\n",
    "      if action.dim() >= 1 and len(action) >=2 :\n",
    "          next_state, reward, done, info = target_env.step(action.cpu().numpy())\n",
    "      else:\n",
    "          next_state, reward, done, info = target_env.step(action.cpu().item())\n",
    "\n",
    "      rewards.append(reward)\n",
    "      entropies.append(model.entropy())\n",
    "      if baseline == 2:\n",
    "        value_baseline.append(model_v(state_t))\n",
    "\n",
    "      log_probs.append(log_prob)\n",
    "      state = next_state\n",
    "\n",
    "    cum_reward = reward_to_go(rewards, gamma=gamma)\n",
    "\n",
    "    loss = torch.zeros(1)\n",
    "    value_loss = torch.zeros(1)\n",
    "      \n",
    "    # fix this for all baselines\n",
    "    for i in range(len(log_probs)):\n",
    "      log_prob = log_probs[i]\n",
    "      r = cum_reward[i]\n",
    "      # weight = torch.zeros(1)\n",
    "      # weight += r\n",
    "      if baseline:\n",
    "        if baseline == 1:\n",
    "          b = compute_baseline(cum_reward, btype=baseline)\n",
    "          r = r -  b\n",
    "        else:\n",
    "          b = value_baseline[i]\n",
    "          r = r -  b\n",
    "      if entropy:\n",
    "        r = r + (temp * entropies[i])\n",
    "      loss -= (log_prob * r.detach())\n",
    "      # print(f\"r={r}\")\n",
    "      # print(f\"log_prob={log_prob}\")\n",
    "      # print(f\"loss={loss}\")\n",
    "      if baseline == 2:\n",
    "        value_loss += (r - b)**2\n",
    "    batch_loss += loss\n",
    "    if baseline == 2:\n",
    "      value_batch_loss += value_loss\n",
    "\n",
    "    rewards_avg += sum(rewards)\n",
    "    entropy_avg += sum(entropies)\n",
    "    max_reward = sum(rewards) if sum(rewards) > max_reward else max_reward\n",
    "    min_reward = sum(rewards) if sum(rewards) < min_reward else min_reward\n",
    "\n",
    "  batch_loss /= batch_size\n",
    "  rewards_avg /= batch_size\n",
    "  entropy_avg /= batch_size\n",
    "\n",
    "  \n",
    "  # optimize the value\n",
    "  if baseline == 2:\n",
    "    value_batch_loss /= batch_size\n",
    "    value_optimizer.zero_grad()\n",
    "    value_batch_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "\n",
    "  # optimize the policy\n",
    "  optimizer.zero_grad()\n",
    "  batch_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "\n",
    "  grad_norm = gradient_norm(model)\n",
    "  if baseline == 2:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward, value_loss=value_batch_loss)\n",
    "  elif baseline == 1:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward, avg_reward_baseline=compute_baseline(cum_reward, btype=baseline))\n",
    "  else:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward)\n",
    "  print(f\"Iteration:{iter+1}, reward: {rewards_avg}, batch loss: {batch_loss.cpu().detach().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run without transfer\n",
    "# training loop\n",
    "# create the environment\n",
    "target_env = gym.make(target_env_name)\n",
    "\n",
    "# create the policy model\n",
    "action_type = \"discrete\" if type(gym.make(target_env_name).action_space) == gym.spaces.Discrete else \"continuous\"\n",
    "if action_type == \"discrete\":\n",
    "  model = MLP_policy(target_env.observation_space.shape[0], [64, 32], target_env.action_space.n, action_type)\n",
    "else:\n",
    "  model = MLP_policy(target_env.observation_space.shape[0], [64, 32], target_env.action_space.shape[0], action_type)\n",
    "# model\n",
    "optimizer = opt.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# if baseline == 2:\n",
    "#   # create the value model\n",
    "#   model_v = MLP_value(env.observation_space.shape[0], [64])\n",
    "#   # create the optimizer\n",
    "#   value_optimizer = opt.Adam(model_v.parameters(), lr=lr)\n",
    "\n",
    "# create a summaryWriter\n",
    "sw = SummaryWriter(comment=f\"env_name:{target_env_name}, lr_={lr}, batch_size={batch_size}, baseline: {baseline}, entropy: {entropy}\")\n",
    "temp = 0.1\n",
    "\n",
    "avg_reward_baseline = None\n",
    "\n",
    "for iter in range(iterations):\n",
    "  rewards_avg = 0\n",
    "  entropy_avg = 0\n",
    "\n",
    "  batch_loss = torch.zeros(1)\n",
    "\n",
    "  if baseline == 2:\n",
    "    value_batch_loss = torch.zeros(1)\n",
    "\n",
    "\n",
    "  max_reward = -1000000\n",
    "  min_reward = 1000000\n",
    "  \n",
    "  for traj in range(batch_size):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    state = target_env.reset()\n",
    "    entropies = []\n",
    "    value_baseline = []\n",
    "    done = False\n",
    "    while not done:\n",
    "      state_t = torch.FloatTensor(state)\n",
    "      action, log_prob = model.sample_action(state_t)\n",
    "\n",
    "      if action.dim() >= 1 and len(action) >=2 :\n",
    "          next_state, reward, done, info = target_env.step(action.cpu().numpy())\n",
    "      else:\n",
    "          next_state, reward, done, info = target_env.step(action.cpu().item())\n",
    "\n",
    "      rewards.append(reward)\n",
    "      entropies.append(model.entropy())\n",
    "      if baseline == 2:\n",
    "        value_baseline.append(model_v(state_t))\n",
    "\n",
    "      log_probs.append(log_prob)\n",
    "      state = next_state\n",
    "\n",
    "    cum_reward = reward_to_go(rewards, gamma=gamma)\n",
    "\n",
    "    loss = torch.zeros(1)\n",
    "    value_loss = torch.zeros(1)\n",
    "      \n",
    "    # fix this for all baselines\n",
    "    for i in range(len(log_probs)):\n",
    "      log_prob = log_probs[i]\n",
    "      r = cum_reward[i]\n",
    "      # weight = torch.zeros(1)\n",
    "      # weight += r\n",
    "      if baseline:\n",
    "        if baseline == 1:\n",
    "          b = compute_baseline(cum_reward, btype=baseline)\n",
    "          r = r -  b\n",
    "        else:\n",
    "          b = value_baseline[i]\n",
    "          r = r -  b\n",
    "      if entropy:\n",
    "        r = r + (temp * entropies[i])\n",
    "      loss -= (log_prob * r.detach())\n",
    "      # print(f\"r={r}\")\n",
    "      # print(f\"log_prob={log_prob}\")\n",
    "      # print(f\"loss={loss}\")\n",
    "      if baseline == 2:\n",
    "        value_loss += (r - b)**2\n",
    "    batch_loss += loss\n",
    "    if baseline == 2:\n",
    "      value_batch_loss += value_loss\n",
    "\n",
    "    rewards_avg += sum(rewards)\n",
    "    entropy_avg += sum(entropies)\n",
    "    max_reward = sum(rewards) if sum(rewards) > max_reward else max_reward\n",
    "    min_reward = sum(rewards) if sum(rewards) < min_reward else min_reward\n",
    "\n",
    "  batch_loss /= batch_size\n",
    "  rewards_avg /= batch_size\n",
    "  entropy_avg /= batch_size\n",
    "\n",
    "  \n",
    "  # optimize the value\n",
    "  if baseline == 2:\n",
    "    value_batch_loss /= batch_size\n",
    "    value_optimizer.zero_grad()\n",
    "    value_batch_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "\n",
    "  # optimize the policy\n",
    "  optimizer.zero_grad()\n",
    "  batch_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "\n",
    "  grad_norm = gradient_norm(model)\n",
    "  if baseline == 2:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward, value_loss=value_batch_loss)\n",
    "  elif baseline == 1:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward, avg_reward_baseline=compute_baseline(cum_reward, btype=baseline))\n",
    "  else:\n",
    "    log_values(sw, iter, reward=rewards_avg, loss=batch_loss, gradient_norm=grad_norm, entropy=entropy_avg, maximum_reward=max_reward, minimum_reward=min_reward)\n",
    "  print(f\"Iteration:{iter+1}, reward: {rewards_avg}, batch loss: {batch_loss.cpu().detach().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
